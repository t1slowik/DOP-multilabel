{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rozpoznawanie typów zgłoszeń DOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import random\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "# from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import fastText\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## przygotowanie danych uczących"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datapath='/mnt/c/dev/DOP-categories/'\n",
    "datafile='Cases categorization.xlsx'\n",
    "dane_surowe=pd.read_excel(os.path.join(datapath,datafile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dane_surowe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dane_surowe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=pd.DataFrame()\n",
    "input_data[['content','category']]=dane_surowe[['case_desc','Unnamed: 15']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing data\n",
    "# remove duplicates\n",
    "input_data.drop_duplicates(inplace=True)\n",
    "# remove empty\n",
    "input_data=input_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [content, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find duplicates\n",
    "input_data[input_data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label freq analysis & selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(raw_labels):\n",
    "    label_list=raw_labels.split(';')\n",
    "    label_list=[x.strip().replace(' ','_') for x in label_list if x != '']\n",
    "    return label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lists=input_data['category'].apply(parse_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "label_count=Counter(chain.from_iterable(label_lists.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(list(label_count.keys()),list(label_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_label_freq=5  # minimum occurencies to be used in \n",
    "pruned_label_count = {key:value for (key,value) in label_count.items() if value>=min_label_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(list(pruned_label_count.keys()),list(pruned_label_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_labels=list(pruned_label_count.keys())\n",
    "pd.Series(interesting_labels).to_csv(os.path.join(datapath,'interesting_labels.csv'),header=['label'],index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing content text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Słownik synonimów / podmian\n",
    "\n",
    "podmiany=pd.read_excel(os.path.join(datapath,'preproc_dict.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(raw_texts,replacements):\n",
    "    \"\"\"\n",
    "    texts: np.Series containing strings to be preprocessed\n",
    "    replacements: pairs of what convert to what\n",
    "    return np.Series with corrected texts\n",
    "    \"\"\"\n",
    "    resulttext=raw_texts.str.lower()\n",
    "    for [co,naco,_] in replacements.values:\n",
    "        resulttext=resulttext.str.replace(re.compile(str(co)),str(naco))\n",
    "    return resulttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_texts=preprocess_texts(input_data['content'],podmiany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'problem z wyświetlaniem informacji w faq w kategorii faq ogólne pierwszy artykuł ma tytuł tytuł dokumentu i treść przykładowy konwent dokumentu login _email msisdn _phonenumber numer zamówienia _number wystąpiło _number _number _number _number _number jira dsd _number'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_labels=pd.read_csv(os.path.join(datapath,'interesting_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(raw_labels, interesting_labels):\n",
    "    result_labels=raw_labels.split(';')  # split on ';'\n",
    "    # remove leading space and replace spaces inside to underscore\n",
    "    result_labels=[x.strip().replace(' ','_') for x in result_labels]\n",
    "    #remove not interesting labels\n",
    "    result_labels=[x for x in result_labels if x in interesting_labels]\n",
    "    return result_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prep_labels=input_data['category'].apply(preprocess_labels, args=(interesting_labels.values,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184                        []\n",
       "185                  [OnFido]\n",
       "186    [payments, activation]\n",
       "187                     [app]\n",
       "188              [UX, layout]\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_labels.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model learning - fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts_train,texts_val,y_train,y_val=train_test_split(prep_texts.values, prep_labels.values,\n",
    "#                                                      test_size=0.25,random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file for fasttext emb\n",
    "# pd.Series(texts_train).to_csv(os.path.join(datapath,'texts_for_emb.txt'),sep='\\n',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling to boost minority classes\n",
    "# ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "# texts_train, y_train = ros.fit_resample(np.reshape(texts_train, (-1, 1)),y_train)\n",
    "\n",
    "# shuffle to be sure \n",
    "# texts_train, y_train = shuffle(texts_train, y_train, random_state=0)\n",
    "\n",
    "# texts_train=texts_train.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare files for fasstext\n",
    "def prepare_fasttext(texts, labels):\n",
    "    fasttext_set=[]\n",
    "    for i,l in enumerate(labels):\n",
    "        l=['__label__'+x for x in l]\n",
    "        labs=' '.join(l) \n",
    "        fasttext_set.append(labs + ' ' + texts[i])\n",
    "    return fasttext_set\n",
    "\n",
    "def prepare_fasttext_file(texts, labels, filename):\n",
    "    fasttext_set=prepare_fasttext(texts, labels)\n",
    "    pd.Series(fasttext_set).to_csv(os.path.join(datapath,filename),sep='\\n',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training & evaluation\n",
    "def train_and_evaluate(texts, labels, params):\n",
    "    print(params)\n",
    "    avg_precision = avg_recall = 0\n",
    "    fold_n=4\n",
    "    folds = KFold(n_splits=fold_n, shuffle=True, random_state=1)\n",
    "    for i, (train_index, val_index) in enumerate(folds.split(texts)):\n",
    "        tic=time.time()\n",
    "        print(f'Calculating fold {i+1}/{fold_n}...')\n",
    "        # Generate batches from indices\n",
    "        texts_train, texts_val = texts[train_index], texts[val_index]\n",
    "        y_train, y_val = labels[train_index], labels[val_index]\n",
    "        # prepare files for train and validation\n",
    "        prepare_fasttext_file(texts_train, y_train, 'fasttext_train.txt')\n",
    "        prepare_fasttext_file(texts_val, y_val, 'fasttext_val.txt')\n",
    "        # train classifier\n",
    "        classifier = fastText.train_supervised(os.path.join(datapath,'fasttext_train.txt'), **params)\n",
    "        # test classifier\n",
    "        supp,precision,recall=classifier.test(os.path.join(datapath,'fasttext_val.txt'), k=2)\n",
    "        f1=2*precision*recall/(precision+recall)\n",
    "        print(f'precision={precision:.2}, recall={recall:.2}, f1={f1:.2}')\n",
    "        avg_precision+=precision/fold_n\n",
    "        avg_recall+=recall/fold_n\n",
    "        toc=time.time()\n",
    "        # print(f'Fold {i+1} calcutated in {toc-tic}.')\n",
    "    avg_f1=2*avg_precision*avg_recall/(avg_precision+avg_recall)\n",
    "    print(f'** Average results: precision={avg_precision:.2}, recall={avg_recall:.2}, f1={avg_f1:.2}')\n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6000, 'dim': 40, 'wordNgrams': 1}\n",
      "Calculating fold 1/4...\n",
      "precision=0.37, recall=0.45, f1=0.4\n",
      "Calculating fold 2/4...\n",
      "precision=0.55, recall=0.64, f1=0.59\n",
      "Calculating fold 3/4...\n",
      "precision=0.47, recall=0.55, f1=0.51\n",
      "Calculating fold 4/4...\n",
      "precision=0.44, recall=0.51, f1=0.47\n",
      "** Average results: precision=0.46, recall=0.54, f1=0.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45691647194095175, 0.5381419037583421, 0.4942140201494178)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(prep_texts.values, prep_labels.values, {'epoch':6000, 'dim':40, 'wordNgrams':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "best_params = {'epoch':6000, 'dim':50, 'wordNgrams':1}\n",
    "prepare_fasttext_file(prep_texts.values, prep_labels.values, 'fasttext_final_train.txt')\n",
    "classifier = fastText.train_supervised(os.path.join(datapath,'fasttext_final_train.txt'),**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model (on TRAIN data): precision=0.81, recall=0.95, f1=0.88\n"
     ]
    }
   ],
   "source": [
    "supp,precision,recall=classifier.test(os.path.join(datapath,'fasttext_final_train.txt'), k=2)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(f'Final model (on TRAIN data): precision={precision:.2}, recall={recall:.2}, f1={f1:.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_model(os.path.join(datapath,'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/float(len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize labels in validation set\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y_train)\n",
    "y_val_bin = mlb.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(clf, texts, threshold=0.5, k=5):\n",
    "    predicted_labels=[]\n",
    "    predicted_probs=[]\n",
    "    for t in texts:\n",
    "        # print(f't: {t}')\n",
    "        cleared_labels=[]\n",
    "        cleared_probs=[]\n",
    "        labels, probs = clf.predict(t,k=k,threshold=0)\n",
    "        for i,l in enumerate(labels):\n",
    "            if probs[i]>=threshold:\n",
    "                cleared_labels.append(l.replace('__label__',''))\n",
    "                cleared_probs.append(probs[i])\n",
    "        predicted_labels.append(cleared_labels)\n",
    "        predicted_probs.append(cleared_probs)\n",
    "    return predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, pred_probs = make_predictions(classifier, texts_val, threshold=0.4, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_preds=mlb.transform(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_val_bin[0])\n",
    "print(y_val_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hamming loss: {hamming_loss(y_val_bin, y_val_preds)}\")\n",
    "print(f\"Hamming score: {hamming_score(y_val_bin, y_val_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext internal test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=classifier.test(os.path.join(datapath,'fasttext_val.txt'),k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=classifier.test_label(os.path.join(datapath,'fasttext_val.txt'),k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_test=pd.read_csv(os.path.join(datapath,'fasttest_test.txt'), header=None)\n",
    "test_set=preprocess_texts(texts_test[0],podmiany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_formatted_predictions(clf, texts):\n",
    "    predictions=[]\n",
    "    for t in texts:\n",
    "        labels, probs = clf.predict(t,k=5,threshold=0.1)\n",
    "        result_line=''\n",
    "        # print(f'labels:{labels}')\n",
    "        # print(f'probs:{probs}')\n",
    "        for i,l in enumerate(labels):\n",
    "            # print(f'i:{i} l:{l} probs[i]:{probs[i]}')\n",
    "            l=l.replace('__label__','')\n",
    "            result_line+=l + ': ' + str(probs[i]) + '; '\n",
    "        # print(f'result_line: {result_line}')\n",
    "        predictions.append(result_line)\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = make_formatted_predictions(classifier, test_set.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(result).to_csv(os.path.join(datapath,'predictions.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
